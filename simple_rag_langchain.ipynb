{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple RAG (Retrieval-Augmented Generation) Implementation with LangChain 1.0+\n",
    "\n",
    "## Overview\n",
    "This notebook demonstrates a complete RAG pipeline using **LangChain 1.0+ with LCEL** (LangChain Expression Language).\n",
    "\n",
    "### What is RAG?\n",
    "RAG combines retrieval of relevant documents with generation from a Large Language Model (LLM):\n",
    "1. **Retrieval**: Find relevant information from a knowledge base\n",
    "2. **Augmentation**: Add retrieved context to the prompt\n",
    "3. **Generation**: LLM generates answers based on the context\n",
    "\n",
    "### Pipeline Flow:\n",
    "```\n",
    "PDF Documents → Load → Split into Chunks → Create Embeddings → Store in Vector DB\n",
    "                                                                         ↓\n",
    "User Query → Retrieve Similar Chunks → Combine with Query → LLM → Answer\n",
    "```\n",
    "\n",
    "### Components Used:\n",
    "- **Document Loader**: PyPDFLoader (for PDF processing)\n",
    "- **Text Splitter**: RecursiveCharacterTextSplitter (smart chunking)\n",
    "- **Embeddings**: OpenAI text-embedding-3-small (vector representations)\n",
    "- **Vector Store**: FAISS (fast similarity search)\n",
    "- **LLM**: OpenAI GPT-4-Turbo or GPT-3.5-Turbo\n",
    "- **Chain Builder**: LCEL (LangChain Expression Language)\n",
    "\n",
    "### LangChain 1.0+ Features:\n",
    "- ✅ Modern LCEL syntax with pipe operator `|`\n",
    "- ✅ More readable and composable chains\n",
    "- ✅ Better streaming support\n",
    "- ✅ Type-safe operations\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installation & Setup\n",
    "\n",
    "First, install all required packages. Make sure you have Python 3.9+ installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# Uncomment and run ONE of the following options:\n",
    "\n",
    "# Option 1: Install from requirements.txt (RECOMMENDED)\n",
    "# !pip install -r requirements.txt\n",
    "\n",
    "# Option 2: Install all packages individually\n",
    "# !pip install langchain langchain-core langchain-openai langchain-community langchain-text-splitters faiss-cpu pypdf python-dotenv tiktoken jupyter notebook\n",
    "\n",
    "# Option 3: Quick install (if you're having import issues)\n",
    "# !pip install --upgrade langchain langchain-core langchain-openai langchain-community langchain-text-splitters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Required Libraries\n",
    "\n",
    "Import all necessary modules with explanations of what each does."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify Installation\n",
    "\n",
    "If you encounter import errors, run this cell first to check package versions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain langchain-core langchain-openai langchain-community langchain-text-splitters faiss-cpu pypdf python-dotenv tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ langchain: 1.0.5\n",
      "✓ langchain-core: 1.0.5\n",
      "✓ langchain-openai: 1.0.3\n",
      "✓ langchain-community: 0.4.1\n",
      "\n",
      "Python version: 3.13.9 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 19:09:58) [MSC v.1929 64 bit (AMD64)]\n",
      "\n",
      "If any packages are missing, run:\n",
      "pip install langchain langchain-core langchain-openai langchain-community langchain-text-splitters faiss-cpu pypdf python-dotenv tiktoken\n"
     ]
    }
   ],
   "source": [
    "# Check installed package versions\n",
    "import sys\n",
    "from importlib.metadata import version\n",
    "\n",
    "try:\n",
    "    import langchain\n",
    "    print(f\"✓ langchain: {langchain.__version__}\")\n",
    "except:\n",
    "    print(\"✗ langchain not installed\")\n",
    "\n",
    "try:\n",
    "    import langchain_core\n",
    "    print(f\"✓ langchain-core: {langchain_core.__version__}\")\n",
    "except:\n",
    "    print(\"✗ langchain-core not installed - REQUIRED!\")\n",
    "    print(\"  Run: pip install langchain-core\")\n",
    "\n",
    "try:\n",
    "    import langchain_openai\n",
    "    print(f\"✓ langchain-openai: {version('langchain-openai')}\")\n",
    "except:\n",
    "    print(\"✗ langchain-openai not installed\")\n",
    "\n",
    "try:\n",
    "    import langchain_community\n",
    "    print(f\"✓ langchain-community: {langchain_community.__version__}\")\n",
    "except:\n",
    "    print(\"✗ langchain-community not installed\")\n",
    "\n",
    "print(f\"\\nPython version: {sys.version}\")\n",
    "print(\"\\nIf any packages are missing, run:\")\n",
    "print(\"pip install langchain langchain-core langchain-openai langchain-community langchain-text-splitters faiss-cpu pypdf python-dotenv tiktoken\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All imports successful!\n",
      "✓ Compatible with LangChain 1.0+\n"
     ]
    }
   ],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "from pathlib import Path\n",
    "\n",
    "# Environment variable management - for secure API key handling\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# LangChain Document Loaders - for loading PDF documents\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "# LangChain Text Splitters - for breaking documents into manageable chunks\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# OpenAI Integration - for embeddings and LLM\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "\n",
    "# Vector Store - FAISS for efficient similarity search\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# LangChain Core Components\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "print(\"✓ All imports successful!\")\n",
    "print(\"✓ Compatible with LangChain 1.0+\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Environment Configuration\n",
    "\n",
    "### Setting up OpenAI API Key\n",
    "\n",
    "You have two options:\n",
    "1. **Recommended**: Create a `.env` file with `OPENAI_API_KEY=your_key_here`\n",
    "2. **Alternative**: Set it directly in code (not recommended for production)\n",
    "\n",
    "Get your API key from: https://platform.openai.com/api-keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ OpenAI API Key loaded successfully!\n",
      "✓ Key starts with: sk-proj-...\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Verify API key is loaded\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    print(\"⚠️  WARNING: OPENAI_API_KEY not found!\")\n",
    "    print(\"Please set it in .env file or uncomment the line below:\")\n",
    "    # os.environ[\"OPENAI_API_KEY\"] = \"your_api_key_here\"\n",
    "else:\n",
    "    print(\"✓ OpenAI API Key loaded successfully!\")\n",
    "    print(f\"✓ Key starts with: {os.getenv('OPENAI_API_KEY')[:8]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Document Loading\n",
    "\n",
    "### Loading PDF Documents\n",
    "\n",
    "PyPDFLoader extracts text from PDF files page by page. Each page becomes a separate document with metadata (page number, source file).\n",
    "\n",
    "**How it works:**\n",
    "- Reads PDF files and extracts text content\n",
    "- Preserves page numbers for source tracking\n",
    "- Returns Document objects with `.page_content` and `.metadata`\n",
    "\n",
    "**Note**: Update the `pdf_path` variable to point to your PDF file(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded 15 pages from 'attention.pdf'\n",
      "\n",
      "--- First Document Preview ---\n",
      "Content (first 500 chars): Provided proper attribution is provided, Google hereby grants permission to\n",
      "reproduce the tables and figures in this paper solely for use in journalistic or\n",
      "scholarly works.\n",
      "Attention Is All You Need\n",
      "Ashish Vaswani∗\n",
      "Google Brain\n",
      "avaswani@google.com\n",
      "Noam Shazeer∗\n",
      "Google Brain\n",
      "noam@google.com\n",
      "Niki Parmar∗\n",
      "Google Research\n",
      "nikip@google.com\n",
      "Jakob Uszkoreit∗\n",
      "Google Research\n",
      "usz@google.com\n",
      "Llion Jones∗\n",
      "Google Research\n",
      "llion@google.com\n",
      "Aidan N. Gomez∗ †\n",
      "University of Toronto\n",
      "aidan@cs.toronto.edu\n",
      "Łukasz ...\n",
      "\n",
      "Metadata: {'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1'}\n",
      "\n",
      "Total characters across all pages: 39,587\n"
     ]
    }
   ],
   "source": [
    "# ===== CONFIGURATION: Update this path to your PDF file =====\n",
    "pdf_path = \"attention.pdf\"  # Change this to your PDF file path\n",
    "# =============================================================\n",
    "\n",
    "# Check if file exists\n",
    "if not os.path.exists(pdf_path):\n",
    "    print(f\"⚠️  ERROR: File '{pdf_path}' not found!\")\n",
    "    print(\"Please update the pdf_path variable with your PDF file location.\")\n",
    "else:\n",
    "    # Initialize the PDF loader\n",
    "    loader = PyPDFLoader(pdf_path)\n",
    "    \n",
    "    # Load all pages from the PDF\n",
    "    # Each page becomes a separate Document object\n",
    "    documents = loader.load()\n",
    "    \n",
    "    # Display information about loaded documents\n",
    "    print(f\"✓ Loaded {len(documents)} pages from '{pdf_path}'\")\n",
    "    print(f\"\\n--- First Document Preview ---\")\n",
    "    print(f\"Content (first 500 chars): {documents[0].page_content[:500]}...\")\n",
    "    print(f\"\\nMetadata: {documents[0].metadata}\")\n",
    "    print(f\"\\nTotal characters across all pages: {sum(len(doc.page_content) for doc in documents):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Multiple PDFs (Optional)\n",
    "\n",
    "If you have multiple PDF files, you can load them all at once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 PDF files\n",
      "  ✓ Loaded 19 pages from rag.pdf\n",
      "  ✓ Loaded 21 pages from ragsurvey.pdf\n",
      "\n",
      "Total pages loaded: 40\n"
     ]
    }
   ],
   "source": [
    "# Example: Loading multiple PDFs from a directory\n",
    "# Uncomment and modify if you want to load multiple files\n",
    "\n",
    "pdf_directory = \"./pdfs\"  # Directory containing your PDFs\n",
    "all_documents = []\n",
    "\n",
    "if os.path.exists(pdf_directory):\n",
    "    pdf_files = list(Path(pdf_directory).glob(\"*.pdf\"))\n",
    "    print(f\"Found {len(pdf_files)} PDF files\")\n",
    "    \n",
    "    for pdf_file in pdf_files:\n",
    "        loader = PyPDFLoader(str(pdf_file))\n",
    "        docs = loader.load()\n",
    "        all_documents.extend(docs)\n",
    "        print(f\"  ✓ Loaded {len(docs)} pages from {pdf_file.name}\")\n",
    "    \n",
    "    print(f\"\\nTotal pages loaded: {len(all_documents)}\")\n",
    "    documents = all_documents  # Use this for the rest of the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Text Splitting\n",
    "\n",
    "### Why Split Documents?\n",
    "- LLMs have token limits (e.g., 4K, 8K, 128K tokens)\n",
    "- Smaller chunks = more precise retrieval\n",
    "- Balance: chunks must be large enough to contain meaningful context but small enough to be specific\n",
    "\n",
    "### RecursiveCharacterTextSplitter\n",
    "This splitter tries to keep related text together by recursively splitting on:\n",
    "1. Paragraphs (`\\n\\n`)\n",
    "2. Lines (`\\n`)\n",
    "3. Sentences (`. `)\n",
    "4. Words (` `)\n",
    "5. Characters (as last resort)\n",
    "\n",
    "**Parameters:**\n",
    "- `chunk_size=1024`: Target size for each chunk (in characters)\n",
    "- `chunk_overlap=128`: Overlap between chunks to maintain context continuity\n",
    "- Overlap prevents important information from being split across chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Split 40 documents into 218 chunks\n",
      "\n",
      "Average chunk size: 904 characters\n",
      "\n",
      "--- Chunk Examples ---\n",
      "\n",
      "Chunk 1 (length: 1008 chars):\n",
      "Retrieval-Augmented Generation for\n",
      "Knowledge-Intensive NLP Tasks\n",
      "Patrick Lewis†‡, Ethan Perez⋆,\n",
      "Aleksandra Piktus†, Fabio Petroni†, Vladimir Karpukhin†, Naman Goyal†, Heinrich Küttler†,\n",
      "Mike Lewis†, W...\n",
      "Metadata: {'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'author': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'pdfs\\\\rag.pdf', 'total_pages': 19, 'page': 0, 'page_label': '1'}\n",
      "\n",
      "Chunk 2 (length: 1023 chars):\n",
      "memory have so far been only investigated for extractive downstream tasks. We\n",
      "explore a general-purpose ﬁne-tuning recipe for retrieval-augmented generation\n",
      "(RAG) — models which combine pre-trained pa...\n",
      "Metadata: {'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'author': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'pdfs\\\\rag.pdf', 'total_pages': 19, 'page': 0, 'page_label': '1'}\n",
      "\n",
      "Chunk 3 (length: 987 chars):\n",
      "more speciﬁc, diverse and factual language than a state-of-the-art parametric-only\n",
      "seq2seq baseline.\n",
      "1 Introduction\n",
      "Pre-trained neural language models have been shown to learn a substantial amount of ...\n",
      "Metadata: {'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'author': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'pdfs\\\\rag.pdf', 'total_pages': 19, 'page': 0, 'page_label': '1'}\n"
     ]
    }
   ],
   "source": [
    "# Initialize the text splitter with recommended settings\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1024,        # Maximum characters per chunk (roughly 200-250 tokens)\n",
    "    chunk_overlap=128,      # Characters overlap between chunks (maintains context)\n",
    "    length_function=len,    # Function to measure chunk length\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]  # Try to split on paragraphs first, then lines, etc.\n",
    ")\n",
    "\n",
    "# Split the documents into chunks\n",
    "# This creates smaller, manageable pieces while preserving semantic meaning\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "# Display splitting results\n",
    "print(f\"✓ Split {len(documents)} documents into {len(chunks)} chunks\")\n",
    "print(f\"\\nAverage chunk size: {sum(len(chunk.page_content) for chunk in chunks) / len(chunks):.0f} characters\")\n",
    "\n",
    "# Preview a few chunks\n",
    "print(f\"\\n--- Chunk Examples ---\")\n",
    "for i, chunk in enumerate(chunks[:3]):\n",
    "    print(f\"\\nChunk {i+1} (length: {len(chunk.page_content)} chars):\")\n",
    "    print(f\"{chunk.page_content[:200]}...\")\n",
    "    print(f\"Metadata: {chunk.metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Creating Embeddings\n",
    "\n",
    "### What are Embeddings?\n",
    "Embeddings are vector representations of text that capture semantic meaning. Similar texts have similar vectors.\n",
    "\n",
    "**Example**: \n",
    "- \"dog\" and \"puppy\" → similar vectors (close in vector space)\n",
    "- \"dog\" and \"spaceship\" → different vectors (far apart)\n",
    "\n",
    "### OpenAI text-embedding-3-small\n",
    "- **Dimensions**: 1536 (each text becomes a 1536-dimensional vector)\n",
    "- **Cost**: $0.00002 per 1,000 tokens (very affordable)\n",
    "- **Performance**: 62.3% on MTEB benchmark\n",
    "- **Speed**: Fast and efficient\n",
    "\n",
    "**Alternative**: `text-embedding-3-large` for higher quality (64.6% MTEB) at higher cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Embeddings model initialized: text-embedding-3-small\n",
      "✓ Embedding dimension: 50\n",
      "✓ Sample embedding (first 10 values): This is a \n",
      "\n",
      "ℹ️  Each chunk will be converted to a 50-dimensional vector for similarity search\n"
     ]
    }
   ],
   "source": [
    "# Initialize OpenAI Embeddings\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-small\",  # Latest, cost-effective embedding model\n",
    "    #dimensions=128\n",
    "    # Alternative: \"text-embedding-3-large\" for better quality\n",
    ")\n",
    "\n",
    "# Test the embeddings with a sample text\n",
    "sample_text = \"This is a test sentence to demonstrate embeddings.\"\n",
    "#sample_embedding = embeddings.embed_query(sample_text)\n",
    "\n",
    "from tenacity import retry, wait_random_exponential, stop_after_attempt\n",
    "\n",
    "@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(6))\n",
    "def embed(sample_text):\n",
    "    return embeddings.embed_query(sample_text)\n",
    "\n",
    "print(f\"✓ Embeddings model initialized: text-embedding-3-small\")\n",
    "print(f\"✓ Embedding dimension: {len(sample_text)}\")\n",
    "print(f\"✓ Sample embedding (first 10 values): {sample_text[:10]}\")\n",
    "print(f\"\\nℹ️  Each chunk will be converted to a {len(sample_text)}-dimensional vector for similarity search\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google Gemini Embeddings (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below is the code for Gemini Embeddings:\n",
    "\n",
    "# from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "# embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "\n",
    "# # Test the embeddings with a sample text\n",
    "# sample_text = \"This is a test sentence to demonstrate embeddings.\"\n",
    "# # sample_embedding = embeddings.embed_query(sample_text)\n",
    "\n",
    "# print(f\"✓ Embeddings model initialized: text-embedding-3-small\")\n",
    "# print(f\"✓ Embedding dimension: {len(sample_embedding)}\")\n",
    "# print(f\"✓ Sample embedding (first 10 values): {sample_embedding[:10]}\")\n",
    "# print(f\"✓ Each chunk will be converted to a {len(sample_embedding)}-dimensional vector for similarity search\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Creating Vector Store (FAISS)\n",
    "\n",
    "### What is a Vector Store?\n",
    "A vector store (or vector database) stores embeddings and enables fast similarity search.\n",
    "\n",
    "### FAISS (Facebook AI Similarity Search)\n",
    "- **Fast**: Optimized for billion-scale vector search\n",
    "- **Local**: Runs on your machine, no cloud dependency\n",
    "- **Efficient**: Uses advanced indexing algorithms\n",
    "\n",
    "### How Similarity Search Works:\n",
    "1. Convert query to embedding vector\n",
    "2. Find vectors in the database most similar to query vector (using cosine similarity or Euclidean distance)\n",
    "3. Return the corresponding text chunks\n",
    "\n",
    "**This cell will:**\n",
    "1. Convert all chunks to embeddings (may take a minute for large documents)\n",
    "2. Build a FAISS index\n",
    "3. Save to disk for future use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FAISS-CPU Version Compatibility Issue\n",
    "\n",
    "If you encounter issues with `faiss-cpu` installation, try:\n",
    "\n",
    "```bash\n",
    "uv pip uninstall faiss-cpu\n",
    "uv pip install faiss-cpu==1.12.0\n",
    "```\n",
    "\n",
    "Or for conda users:\n",
    "```bash\n",
    "conda install -c conda-forge faiss-cpu==1.12.0\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: faiss-cpu\n",
      "Version: 1.12.0\n",
      "Summary: A library for efficient similarity search and clustering of dense vectors.\n",
      "Home-page: \n",
      "Author: \n",
      "Author-email: Kota Yamaguchi <yamaguchi_kota@cyberagent.co.jp>\n",
      "License-Expression: MIT AND BSD-3-Clause\n",
      "Location: C:\\Users\\exact\\anaconda3\\envs\\myenv\\Lib\\site-packages\n",
      "Requires: numpy, packaging\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating FAISS index from 402 chunks...\n",
      "This may take a minute depending on the number of chunks...\n"
     ]
    },
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRateLimitError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCreating FAISS index from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(chunks)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m chunks...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mThis may take a minute depending on the number of chunks...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m vectorstore = \u001b[43mFAISS\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_documents\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;66;43;03m# Our split document chunks\u001b[39;49;00m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43membedding\u001b[49m\u001b[43m=\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;66;43;03m# OpenAI embedding model\u001b[39;49;00m\n\u001b[32m      9\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m✓ FAISS vector store created successfully!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m✓ Indexed \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(chunks)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m document chunks\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\exact\\anaconda3\\envs\\myenv\\Lib\\site-packages\\langchain_core\\vectorstores\\base.py:807\u001b[39m, in \u001b[36mVectorStore.from_documents\u001b[39m\u001b[34m(cls, documents, embedding, **kwargs)\u001b[39m\n\u001b[32m    804\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(ids):\n\u001b[32m    805\u001b[39m         kwargs[\u001b[33m\"\u001b[39m\u001b[33mids\u001b[39m\u001b[33m\"\u001b[39m] = ids\n\u001b[32m--> \u001b[39m\u001b[32m807\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfrom_texts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\exact\\anaconda3\\envs\\myenv\\Lib\\site-packages\\langchain_community\\vectorstores\\faiss.py:1043\u001b[39m, in \u001b[36mFAISS.from_texts\u001b[39m\u001b[34m(cls, texts, embedding, metadatas, ids, **kwargs)\u001b[39m\n\u001b[32m   1016\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[32m   1017\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfrom_texts\u001b[39m(\n\u001b[32m   1018\u001b[39m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1023\u001b[39m     **kwargs: Any,\n\u001b[32m   1024\u001b[39m ) -> FAISS:\n\u001b[32m   1025\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Construct FAISS wrapper from raw documents.\u001b[39;00m\n\u001b[32m   1026\u001b[39m \n\u001b[32m   1027\u001b[39m \u001b[33;03m    This is a user friendly interface that:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1041\u001b[39m \u001b[33;03m            faiss = FAISS.from_texts(texts, embeddings)\u001b[39;00m\n\u001b[32m   1042\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1043\u001b[39m     embeddings = \u001b[43membedding\u001b[49m\u001b[43m.\u001b[49m\u001b[43membed_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1044\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m.__from(\n\u001b[32m   1045\u001b[39m         texts,\n\u001b[32m   1046\u001b[39m         embeddings,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1050\u001b[39m         **kwargs,\n\u001b[32m   1051\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\exact\\anaconda3\\envs\\myenv\\Lib\\site-packages\\langchain_openai\\embeddings\\base.py:702\u001b[39m, in \u001b[36mOpenAIEmbeddings.embed_documents\u001b[39m\u001b[34m(self, texts, chunk_size, **kwargs)\u001b[39m\n\u001b[32m    699\u001b[39m \u001b[38;5;66;03m# Unconditionally call _get_len_safe_embeddings to handle length safety.\u001b[39;00m\n\u001b[32m    700\u001b[39m \u001b[38;5;66;03m# This could be optimized to avoid double work when all texts are short enough.\u001b[39;00m\n\u001b[32m    701\u001b[39m engine = cast(\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mself\u001b[39m.deployment)\n\u001b[32m--> \u001b[39m\u001b[32m702\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_len_safe_embeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    703\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    704\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\exact\\anaconda3\\envs\\myenv\\Lib\\site-packages\\langchain_openai\\embeddings\\base.py:569\u001b[39m, in \u001b[36mOpenAIEmbeddings._get_len_safe_embeddings\u001b[39m\u001b[34m(self, texts, engine, chunk_size, **kwargs)\u001b[39m\n\u001b[32m    567\u001b[39m \u001b[38;5;66;03m# Make API call with this batch\u001b[39;00m\n\u001b[32m    568\u001b[39m batch_tokens = tokens[i:batch_end]\n\u001b[32m--> \u001b[39m\u001b[32m569\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m=\u001b[49m\u001b[43mbatch_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mclient_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    570\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, \u001b[38;5;28mdict\u001b[39m):\n\u001b[32m    571\u001b[39m     response = response.model_dump()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\exact\\anaconda3\\envs\\myenv\\Lib\\site-packages\\openai\\resources\\embeddings.py:132\u001b[39m, in \u001b[36mEmbeddings.create\u001b[39m\u001b[34m(self, input, model, dimensions, encoding_format, user, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    126\u001b[39m             embedding.embedding = np.frombuffer(  \u001b[38;5;66;03m# type: ignore[no-untyped-call]\u001b[39;00m\n\u001b[32m    127\u001b[39m                 base64.b64decode(data), dtype=\u001b[33m\"\u001b[39m\u001b[33mfloat32\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    128\u001b[39m             ).tolist()\n\u001b[32m    130\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\n\u001b[32m--> \u001b[39m\u001b[32m132\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    133\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/embeddings\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    134\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEmbeddingCreateParams\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    135\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    136\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    137\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    140\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpost_parser\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCreateEmbeddingResponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    143\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\exact\\anaconda3\\envs\\myenv\\Lib\\site-packages\\openai\\_base_client.py:1259\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1245\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1246\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1247\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1254\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1255\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1256\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1257\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1258\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1259\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\exact\\anaconda3\\envs\\myenv\\Lib\\site-packages\\openai\\_base_client.py:1047\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1044\u001b[39m             err.response.read()\n\u001b[32m   1046\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1047\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1049\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1051\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mRateLimitError\u001b[39m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
     ]
    }
   ],
   "source": [
    "# Create FAISS vector store from document chunks\n",
    "# This step converts each chunk to an embedding and stores it\n",
    "print(f\"Creating FAISS index from {len(chunks)} chunks...\")\n",
    "print(\"This may take a minute depending on the number of chunks...\")\n",
    "\n",
    "vectorstore = FAISS.from_documents(\n",
    "    documents=chunks,      # Our split document chunks\n",
    "    embedding=embeddings   # OpenAI embedding model\n",
    ")\n",
    "\n",
    "print(f\"✓ FAISS vector store created successfully!\")\n",
    "print(f\"✓ Indexed {len(chunks)} document chunks\")\n",
    "\n",
    "# Save the vector store to disk for later use\n",
    "# This allows you to reload the index without re-processing documents\n",
    "vectorstore_path = \"./faiss_index\"\n",
    "vectorstore.save_local(vectorstore_path)\n",
    "print(f\"✓ Vector store saved to '{vectorstore_path}'\")\n",
    "print(f\"\\nℹ️  You can reload this index later using: FAISS.load_local('{vectorstore_path}', embeddings)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding batch 0 → 50 ...\n"
     ]
    },
    {
     "ename": "RetryError",
     "evalue": "RetryError[<Future at 0x21c3becdf40 state=finished raised RateLimitError>]",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRateLimitError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\exact\\anaconda3\\envs\\myenv\\Lib\\site-packages\\tenacity\\__init__.py:480\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    479\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m480\u001b[39m     result = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    481\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 5\u001b[39m, in \u001b[36mget_embeddings\u001b[39m\u001b[34m(texts)\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;129m@retry\u001b[39m(wait=wait_random_exponential(\u001b[38;5;28mmin\u001b[39m=\u001b[32m1\u001b[39m, \u001b[38;5;28mmax\u001b[39m=\u001b[32m60\u001b[39m), stop=stop_after_attempt(\u001b[32m6\u001b[39m))\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_embeddings\u001b[39m(texts):\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43membeddings\u001b[49m\u001b[43m.\u001b[49m\u001b[43membed_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\exact\\anaconda3\\envs\\myenv\\Lib\\site-packages\\langchain_openai\\embeddings\\base.py:702\u001b[39m, in \u001b[36mOpenAIEmbeddings.embed_documents\u001b[39m\u001b[34m(self, texts, chunk_size, **kwargs)\u001b[39m\n\u001b[32m    701\u001b[39m engine = cast(\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mself\u001b[39m.deployment)\n\u001b[32m--> \u001b[39m\u001b[32m702\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_len_safe_embeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    703\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    704\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\exact\\anaconda3\\envs\\myenv\\Lib\\site-packages\\langchain_openai\\embeddings\\base.py:569\u001b[39m, in \u001b[36mOpenAIEmbeddings._get_len_safe_embeddings\u001b[39m\u001b[34m(self, texts, engine, chunk_size, **kwargs)\u001b[39m\n\u001b[32m    568\u001b[39m batch_tokens = tokens[i:batch_end]\n\u001b[32m--> \u001b[39m\u001b[32m569\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m=\u001b[49m\u001b[43mbatch_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mclient_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    570\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, \u001b[38;5;28mdict\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\exact\\anaconda3\\envs\\myenv\\Lib\\site-packages\\openai\\resources\\embeddings.py:132\u001b[39m, in \u001b[36mEmbeddings.create\u001b[39m\u001b[34m(self, input, model, dimensions, encoding_format, user, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    130\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\n\u001b[32m--> \u001b[39m\u001b[32m132\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    133\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/embeddings\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    134\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEmbeddingCreateParams\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    135\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    136\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    137\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    140\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpost_parser\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCreateEmbeddingResponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    143\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\exact\\anaconda3\\envs\\myenv\\Lib\\site-packages\\openai\\_base_client.py:1259\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1256\u001b[39m opts = FinalRequestOptions.construct(\n\u001b[32m   1257\u001b[39m     method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1258\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1259\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\exact\\anaconda3\\envs\\myenv\\Lib\\site-packages\\openai\\_base_client.py:1047\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1046\u001b[39m     log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1047\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1049\u001b[39m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[31mRateLimitError\u001b[39m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mRetryError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     12\u001b[39m batch = [c.page_content \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m chunks[i:i+BATCH_SIZE]]\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEmbedding batch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m → \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;250m \u001b[39m+\u001b[38;5;250m \u001b[39m\u001b[38;5;28mlen\u001b[39m(batch)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m ...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m batch_vectors = \u001b[43mget_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m all_vectors.extend(batch_vectors)\n\u001b[32m     16\u001b[39m time.sleep(\u001b[32m0.5\u001b[39m)  \u001b[38;5;66;03m# cool down slightly\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\exact\\anaconda3\\envs\\myenv\\Lib\\site-packages\\tenacity\\__init__.py:338\u001b[39m, in \u001b[36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[39m\u001b[34m(*args, **kw)\u001b[39m\n\u001b[32m    336\u001b[39m copy = \u001b[38;5;28mself\u001b[39m.copy()\n\u001b[32m    337\u001b[39m wrapped_f.statistics = copy.statistics  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m338\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\exact\\anaconda3\\envs\\myenv\\Lib\\site-packages\\tenacity\\__init__.py:477\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    475\u001b[39m retry_state = RetryCallState(retry_object=\u001b[38;5;28mself\u001b[39m, fn=fn, args=args, kwargs=kwargs)\n\u001b[32m    476\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m477\u001b[39m     do = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    478\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[32m    479\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\exact\\anaconda3\\envs\\myenv\\Lib\\site-packages\\tenacity\\__init__.py:378\u001b[39m, in \u001b[36mBaseRetrying.iter\u001b[39m\u001b[34m(self, retry_state)\u001b[39m\n\u001b[32m    376\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    377\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter_state.actions:\n\u001b[32m--> \u001b[39m\u001b[32m378\u001b[39m     result = \u001b[43maction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    379\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\exact\\anaconda3\\envs\\myenv\\Lib\\site-packages\\tenacity\\__init__.py:421\u001b[39m, in \u001b[36mBaseRetrying._post_stop_check_actions.<locals>.exc_check\u001b[39m\u001b[34m(rs)\u001b[39m\n\u001b[32m    419\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.reraise:\n\u001b[32m    420\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m retry_exc.reraise()\n\u001b[32m--> \u001b[39m\u001b[32m421\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m retry_exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfut\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexception\u001b[39;00m()\n",
      "\u001b[31mRetryError\u001b[39m: RetryError[<Future at 0x21c3becdf40 state=finished raised RateLimitError>]"
     ]
    }
   ],
   "source": [
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(6))\n",
    "def get_embeddings(texts):\n",
    "    return embeddings.embed_documents(texts)\n",
    "\n",
    "# Batch size (adjust depending on limits — smaller is safer)\n",
    "BATCH_SIZE = 50\n",
    "\n",
    "all_vectors = []\n",
    "for i in range(0, len(chunks), BATCH_SIZE):\n",
    "    batch = [c.page_content for c in chunks[i:i+BATCH_SIZE]]\n",
    "    print(f\"Embedding batch {i} → {i + len(batch)} ...\")\n",
    "    batch_vectors = get_embeddings(batch)\n",
    "    all_vectors.extend(batch_vectors)\n",
    "    time.sleep(0.5)  # cool down slightly\n",
    "\n",
    "vectorstore = FAISS.from_existing_vectors(all_vectors, [c.metadata for c in chunks])\n",
    "vectorstore.save_local(\"./faiss_index\")\n",
    "\n",
    "print(\"✓ Vector store created & saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating FAISS index from 218 chunks...\n",
      "This may take a minute depending on the number of chunks...\n"
     ]
    },
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRateLimitError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCreating FAISS index from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(chunks)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m chunks...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mThis may take a minute depending on the number of chunks...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m vectorstore = \u001b[43mFAISS\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_documents\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;66;43;03m# Our split document chunks\u001b[39;49;00m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43membedding\u001b[49m\u001b[43m=\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;66;43;03m# OpenAI embedding model\u001b[39;49;00m\n\u001b[32m      9\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m✓ FAISS vector store created successfully!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m✓ Indexed \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(chunks)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m document chunks\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\exact\\anaconda3\\envs\\myenv\\Lib\\site-packages\\langchain_core\\vectorstores\\base.py:807\u001b[39m, in \u001b[36mVectorStore.from_documents\u001b[39m\u001b[34m(cls, documents, embedding, **kwargs)\u001b[39m\n\u001b[32m    804\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(ids):\n\u001b[32m    805\u001b[39m         kwargs[\u001b[33m\"\u001b[39m\u001b[33mids\u001b[39m\u001b[33m\"\u001b[39m] = ids\n\u001b[32m--> \u001b[39m\u001b[32m807\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfrom_texts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\exact\\anaconda3\\envs\\myenv\\Lib\\site-packages\\langchain_community\\vectorstores\\faiss.py:1043\u001b[39m, in \u001b[36mFAISS.from_texts\u001b[39m\u001b[34m(cls, texts, embedding, metadatas, ids, **kwargs)\u001b[39m\n\u001b[32m   1016\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[32m   1017\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfrom_texts\u001b[39m(\n\u001b[32m   1018\u001b[39m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1023\u001b[39m     **kwargs: Any,\n\u001b[32m   1024\u001b[39m ) -> FAISS:\n\u001b[32m   1025\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Construct FAISS wrapper from raw documents.\u001b[39;00m\n\u001b[32m   1026\u001b[39m \n\u001b[32m   1027\u001b[39m \u001b[33;03m    This is a user friendly interface that:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1041\u001b[39m \u001b[33;03m            faiss = FAISS.from_texts(texts, embeddings)\u001b[39;00m\n\u001b[32m   1042\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1043\u001b[39m     embeddings = \u001b[43membedding\u001b[49m\u001b[43m.\u001b[49m\u001b[43membed_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1044\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m.__from(\n\u001b[32m   1045\u001b[39m         texts,\n\u001b[32m   1046\u001b[39m         embeddings,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1050\u001b[39m         **kwargs,\n\u001b[32m   1051\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\exact\\anaconda3\\envs\\myenv\\Lib\\site-packages\\langchain_openai\\embeddings\\base.py:702\u001b[39m, in \u001b[36mOpenAIEmbeddings.embed_documents\u001b[39m\u001b[34m(self, texts, chunk_size, **kwargs)\u001b[39m\n\u001b[32m    699\u001b[39m \u001b[38;5;66;03m# Unconditionally call _get_len_safe_embeddings to handle length safety.\u001b[39;00m\n\u001b[32m    700\u001b[39m \u001b[38;5;66;03m# This could be optimized to avoid double work when all texts are short enough.\u001b[39;00m\n\u001b[32m    701\u001b[39m engine = cast(\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mself\u001b[39m.deployment)\n\u001b[32m--> \u001b[39m\u001b[32m702\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_len_safe_embeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    703\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    704\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\exact\\anaconda3\\envs\\myenv\\Lib\\site-packages\\langchain_openai\\embeddings\\base.py:569\u001b[39m, in \u001b[36mOpenAIEmbeddings._get_len_safe_embeddings\u001b[39m\u001b[34m(self, texts, engine, chunk_size, **kwargs)\u001b[39m\n\u001b[32m    567\u001b[39m \u001b[38;5;66;03m# Make API call with this batch\u001b[39;00m\n\u001b[32m    568\u001b[39m batch_tokens = tokens[i:batch_end]\n\u001b[32m--> \u001b[39m\u001b[32m569\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m=\u001b[49m\u001b[43mbatch_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mclient_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    570\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, \u001b[38;5;28mdict\u001b[39m):\n\u001b[32m    571\u001b[39m     response = response.model_dump()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\exact\\anaconda3\\envs\\myenv\\Lib\\site-packages\\openai\\resources\\embeddings.py:132\u001b[39m, in \u001b[36mEmbeddings.create\u001b[39m\u001b[34m(self, input, model, dimensions, encoding_format, user, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    126\u001b[39m             embedding.embedding = np.frombuffer(  \u001b[38;5;66;03m# type: ignore[no-untyped-call]\u001b[39;00m\n\u001b[32m    127\u001b[39m                 base64.b64decode(data), dtype=\u001b[33m\"\u001b[39m\u001b[33mfloat32\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    128\u001b[39m             ).tolist()\n\u001b[32m    130\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\n\u001b[32m--> \u001b[39m\u001b[32m132\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    133\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/embeddings\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    134\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEmbeddingCreateParams\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    135\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    136\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    137\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    140\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpost_parser\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCreateEmbeddingResponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    143\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\exact\\anaconda3\\envs\\myenv\\Lib\\site-packages\\openai\\_base_client.py:1259\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1245\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1246\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1247\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1254\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1255\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1256\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1257\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1258\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1259\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\exact\\anaconda3\\envs\\myenv\\Lib\\site-packages\\openai\\_base_client.py:1047\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1044\u001b[39m             err.response.read()\n\u001b[32m   1046\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1047\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1049\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1051\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mRateLimitError\u001b[39m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
     ]
    }
   ],
   "source": [
    "# Create FAISS vector store from document chunks\n",
    "# This step converts each chunk to an embedding and stores it\n",
    "print(f\"Creating FAISS index from {len(chunks)} chunks...\")\n",
    "print(\"This may take a minute depending on the number of chunks...\")\n",
    "\n",
    "vectorstore = FAISS.from_documents(\n",
    "    documents=chunks,      # Our split document chunks\n",
    "    embedding=embeddings   # OpenAI embedding model\n",
    ")\n",
    "\n",
    "print(f\"✓ FAISS vector store created successfully!\")\n",
    "print(f\"✓ Indexed {len(chunks)} document chunks\")\n",
    "\n",
    "# Save the vector store to disk for later use\n",
    "# This allows you to reload the index without re-processing documents\n",
    "vectorstore_path = \"./faiss_index\"\n",
    "vectorstore.save_local(vectorstore_path)\n",
    "print(f\"✓ Vector store saved to '{vectorstore_path}'\")\n",
    "print(f\"\\nℹ️  You can reload this index later using: FAISS.load_local('{vectorstore_path}', embeddings)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ChromaDB Vector Store (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating ChromaDB from 49 chunks...\n",
      "✓ ChromaDB vector store created!\n"
     ]
    }
   ],
   "source": [
    "#ChromaDB has better Python 3.13 support. Replace cells 21-24 with:\n",
    "\n",
    "# Instead of FAISS, use ChromaDB\n",
    "# from langchain_community.vectorstores import Chroma\n",
    "\n",
    "# # Create ChromaDB vector store\n",
    "# print(f\"Creating ChromaDB from {len(chunks)} chunks...\")\n",
    "# vectorstore = Chroma.from_documents(\n",
    "#     documents=chunks,\n",
    "#     embedding=embeddings,\n",
    "#     persist_directory=\"./chroma_db\"\n",
    "# )\n",
    "# print(\"✓ ChromaDB vector store created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading a Saved Vector Store (Optional)\n",
    "\n",
    "If you've already created a vector store, you can load it instead of recreating:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Uncomment to load an existing vector store instead of creating a new one\n",
    "# vectorstore_path = \"./faiss_index\"\n",
    "# vectorstore = FAISS.load_local(\n",
    "#     vectorstore_path, \n",
    "#     embeddings,\n",
    "#     allow_dangerous_deserialization=True  # Required for loading pickled data\n",
    "# )\n",
    "# print(f\"✓ Loaded existing vector store from '{vectorstore_path}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vectorstore' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Create a retriever from the vector store\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m retriever = \u001b[43mvectorstore\u001b[49m.as_retriever(\n\u001b[32m      3\u001b[39m     search_type=\u001b[33m\"\u001b[39m\u001b[33msimilarity\u001b[39m\u001b[33m\"\u001b[39m,    \u001b[38;5;66;03m# Use cosine similarity for search\u001b[39;00m\n\u001b[32m      4\u001b[39m     search_kwargs={\u001b[33m\"\u001b[39m\u001b[33mk\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m4\u001b[39m}        \u001b[38;5;66;03m# Retrieve top 4 most relevant chunks\u001b[39;00m\n\u001b[32m      5\u001b[39m )\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m✓ Retriever configured successfully\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  - Search type: similarity\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'vectorstore' is not defined"
     ]
    }
   ],
   "source": [
    "# Create a retriever from the vector store\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",    # Use cosine similarity for search\n",
    "    search_kwargs={\"k\": 4}        # Retrieve top 4 most relevant chunks\n",
    ")\n",
    "\n",
    "print(\"✓ Retriever configured successfully\")\n",
    "print(f\"  - Search type: similarity\")\n",
    "print(f\"  - Number of documents to retrieve (k): 4\")\n",
    "\n",
    "# Test the retriever with a sample query\n",
    "# Note: In LangChain 1.0+, use .invoke() instead of .get_relevant_documents()\n",
    "test_query = \"What is the main topic of this document?\"\n",
    "retrieved_docs = retriever.invoke(test_query)  # LangChain 1.0+ method\n",
    "\n",
    "print(f\"\\n--- Retriever Test ---\")\n",
    "print(f\"Query: '{test_query}'\")\n",
    "print(f\"Retrieved {len(retrieved_docs)} documents:\")\n",
    "\n",
    "for i, doc in enumerate(retrieved_docs):\n",
    "    print(f\"\\nDocument {i+1}:\")\n",
    "    print(f\"  Content preview: {doc.page_content[:150]}...\")\n",
    "    print(f\"  Metadata: {doc.metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Configuring the Language Model (LLM)\n",
    "\n",
    "### LLM Selection\n",
    "The LLM generates the final answer based on retrieved context.\n",
    "\n",
    "### Available Models:\n",
    "1. **gpt-4-turbo-2025-04-09**: Most capable, best quality, slower, more expensive\n",
    "2. **gpt-4o**: Fast GPT-4 level performance, good balance\n",
    "3. **gpt-3.5-turbo**: Fast and cheap, good for simpler queries\n",
    "\n",
    "### Temperature:\n",
    "- **0**: Deterministic, focused answers (recommended for factual Q&A)\n",
    "- **0.7**: More creative, varied responses\n",
    "- **1.0**: Most creative, less predictable\n",
    "\n",
    "### Max Tokens:\n",
    "Controls the maximum length of the generated response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ LLM configured successfully\n",
      "  - Model: gpt-4-turbo-2024-04-09\n",
      "  - Temperature: 0 (deterministic)\n",
      "  - Max tokens: 2000\n",
      "\n",
      "LLM Test Response: Hello, I am ready to answer questions!\n"
     ]
    }
   ],
   "source": [
    "# Initialize the ChatOpenAI model\n",
    "llm = ChatOpenAI(\n",
    "      model=\"gpt-4-turbo-2024-04-09\",  # Choose your model\n",
    "      # Alternative options:\n",
    "      # model=\"gpt-4o\",           # Faster GPT-4 performance, good \n",
    "      # balance,\n",
    "      # model=\"gpt-3.5-turbo\",    # Faster and cheaper option\n",
    "\n",
    "      temperature=0,         # 0 = deterministic, factual responses (recommended for Q&A)\n",
    "      max_tokens=2000,       # Maximum length of response\n",
    "  )\n",
    "\n",
    "print(\"✓ LLM configured successfully\")\n",
    "print(f\"  - Model: gpt-4-turbo-2024-04-09\")\n",
    "print(f\"  - Temperature: 0 (deterministic)\")\n",
    "print(f\"  - Max tokens: 2000\")\n",
    "\n",
    "# Test the LLM with a simple query\n",
    "test_response = llm.invoke(\"Say 'Hello, I am ready to answer questions!'\")\n",
    "print(f\"\\nLLM Test Response: {test_response.content}\")\n",
    "\n",
    "#   📝 Explanation of Parameters:\n",
    "\n",
    "#   Model Selection:\n",
    "\n",
    "#   # Option 1: Best quality (slower, more expensive)\n",
    "#   llm = ChatOpenAI(model=\"gpt-4-turbo-2024-04-09\")\n",
    "\n",
    "#   # Option 2: Fast GPT-4 performance (balanced)\n",
    "#   llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "\n",
    "#   # Option 3: Fast and cheap (good for testing)\n",
    "#   llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "#   Temperature:\n",
    "\n",
    "#   temperature=0    # Deterministic, focused (best for factual Q&A)\n",
    "#   temperature=0.7  # More creative, varied responses\n",
    "#   temperature=1.0  # Most creative, less predictable\n",
    "\n",
    "#   Max Tokens:\n",
    "\n",
    "#   max_tokens=2000  # Controls maximum response length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Creating the RAG Chain (LangChain 1.0+ LCEL)\n",
    "\n",
    "### What is a RAG Chain?\n",
    "The RAG chain combines retrieval and generation into a single workflow:\n",
    "1. User asks a question\n",
    "2. Retriever finds relevant documents\n",
    "3. Documents are formatted as context\n",
    "4. LLM generates answer using the context\n",
    "\n",
    "### LangChain 1.0+ LCEL (LangChain Expression Language)\n",
    "LangChain 1.0+ uses LCEL, a declarative way to build chains using the pipe operator `|`.\n",
    "\n",
    "**Benefits:**\n",
    "- More intuitive and readable\n",
    "- Better streaming support\n",
    "- Easier to debug and modify\n",
    "- Type-safe and composable\n",
    "\n",
    "**Components:**\n",
    "- **RunnablePassthrough**: Passes input through unchanged\n",
    "- **Pipe operator (|)**: Chains components together\n",
    "- **StrOutputParser**: Converts LLM output to string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Retriever configured successfully\n",
      "  - Search type: similarity\n",
      "  - Number of documents to retrieve (k): 4\n",
      "\n",
      "--- Retriever Test ---\n",
      "Query: 'What is the main topic of this document?'\n",
      "Retrieved 4 documents:\n",
      "\n",
      "Document 1:\n",
      "  Content preview: Attention Visualizations\n",
      "Input-Input Layer5\n",
      "It\n",
      "is\n",
      "in\n",
      "this\n",
      "spirit\n",
      "that\n",
      "a\n",
      "majority\n",
      "of\n",
      "American\n",
      "governments\n",
      "have\n",
      "passed\n",
      "new\n",
      "laws\n",
      "since\n",
      "2009\n",
      "making\n",
      "the\n",
      "re...\n",
      "  Metadata: {'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 12, 'page_label': '13'}\n",
      "\n",
      "Document 2:\n",
      "  Content preview: Input-Input Layer5\n",
      "The\n",
      "Law\n",
      "will\n",
      "never\n",
      "be\n",
      "perfect\n",
      ",\n",
      "but\n",
      "its\n",
      "application\n",
      "should\n",
      "be\n",
      "just\n",
      "-\n",
      "this\n",
      "is\n",
      "what\n",
      "we\n",
      "are\n",
      "missing\n",
      ",\n",
      "in\n",
      "my\n",
      "opinion\n",
      ".\n",
      "<EOS>\n",
      "<pad>\n",
      "The\n",
      "...\n",
      "  Metadata: {'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 13, 'page_label': '14'}\n",
      "\n",
      "Document 3:\n",
      "  Content preview: Input-Input Layer5\n",
      "The\n",
      "Law\n",
      "will\n",
      "never\n",
      "be\n",
      "perfect\n",
      ",\n",
      "but\n",
      "its\n",
      "application\n",
      "should\n",
      "be\n",
      "just\n",
      "-\n",
      "this\n",
      "is\n",
      "what\n",
      "we\n",
      "are\n",
      "missing\n",
      ",\n",
      "in\n",
      "my\n",
      "opinion\n",
      ".\n",
      "<EOS>\n",
      "<pad>\n",
      "The\n",
      "...\n",
      "  Metadata: {'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 14, 'page_label': '15'}\n",
      "\n",
      "Document 4:\n",
      "  Content preview: translation system: Bridging the gap between human and machine translation. arXiv preprint\n",
      "arXiv:1609.08144, 2016.\n",
      "[39] Jie Zhou, Ying Cao, Xuguang Wa...\n",
      "  Metadata: {'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 11, 'page_label': '12'}\n"
     ]
    }
   ],
   "source": [
    "#Step 1: Create the Retriever \n",
    "# Create a retriever from the vector store\n",
    "retriever = vectorstore.as_retriever(\n",
    "      search_type=\"similarity\",    # Use cosine similarity for search\n",
    "      search_kwargs={\"k\": 4}        # Retrieve top 4 most relevant chunks\n",
    "  )\n",
    "\n",
    "print(\"✓ Retriever configured successfully\")\n",
    "print(f\"  - Search type: similarity\")\n",
    "print(f\"  - Number of documents to retrieve (k): 4\")\n",
    "\n",
    "  # Test the retriever with a sample query\n",
    "test_query = \"What is the main topic of this document?\"\n",
    "retrieved_docs = retriever.invoke(test_query)\n",
    "\n",
    "print(f\"\\n--- Retriever Test ---\")\n",
    "print(f\"Query: '{test_query}'\")\n",
    "print(f\"Retrieved {len(retrieved_docs)} documents:\")\n",
    "\n",
    "for i, doc in enumerate(retrieved_docs):\n",
    "    print(f\"\\nDocument {i+1}:\")\n",
    "    print(f\"  Content preview: {doc.page_content[:150]}...\")\n",
    "    print(f\"  Metadata: {doc.metadata}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ RAG chain created successfully using LangChain 1.0+ LCEL!\n",
      "\n",
      "RAG Pipeline Flow:\n",
      "  1. User provides a query\n",
      "  2. Retriever finds top 4 relevant chunks\n",
      "  3. Chunks are formatted as context\n",
      "  4. Context + question are formatted with prompt template\n",
      "  5. LLM generates answer based on context\n",
      "  6. Answer is parsed and returned to user\n"
     ]
    }
   ],
   "source": [
    "# Define the prompt template for the RAG system\n",
    "# This tells the LLM how to use the retrieved context\n",
    "system_prompt = (\n",
    "    \"You are a helpful assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer the question. \"\n",
    "    \"If you don't know the answer based on the context, say that you don't know. \"\n",
    "    \"Keep the answer concise and accurate.\\n\\n\"\n",
    "    \"Context: {context}\\n\\n\"\n",
    "    \"Question: {question}\"\n",
    ")\n",
    "\n",
    "# Create the prompt template\n",
    "prompt = ChatPromptTemplate.from_template(system_prompt)\n",
    "\n",
    "# Helper function to format documents\n",
    "def format_docs(docs):\n",
    "    \"\"\"Format retrieved documents into a single string.\"\"\"\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Build the RAG chain using LangChain 1.0+ LCEL (LangChain Expression Language)\n",
    "# This uses the pipe operator (|) to chain components together\n",
    "rag_chain = (\n",
    "    {\n",
    "        \"context\": retriever | format_docs,  # Retrieve docs and format them\n",
    "        \"question\": RunnablePassthrough()      # Pass through the question\n",
    "    }\n",
    "    | prompt           # Format with prompt template\n",
    "    | llm              # Generate answer with LLM\n",
    "    | StrOutputParser() # Parse output to string\n",
    ")\n",
    "\n",
    "print(\"✓ RAG chain created successfully using LangChain 1.0+ LCEL!\")\n",
    "print(\"\\nRAG Pipeline Flow:\")\n",
    "print(\"  1. User provides a query\")\n",
    "print(\"  2. Retriever finds top 4 relevant chunks\")\n",
    "print(\"  3. Chunks are formatted as context\")\n",
    "print(\"  4. Context + question are formatted with prompt template\")\n",
    "print(\"  5. LLM generates answer based on context\")\n",
    "print(\"  6. Answer is parsed and returned to user\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is the main topic or subject of this document?\n",
      "\n",
      "Processing...\n",
      "\n",
      "================================================================================\n",
      "ANSWER:\n",
      "================================================================================\n",
      "The main topic of the document is the introduction and explanation of the Transformer network architecture, which is based solely on attention mechanisms and does not use recurrence or convolutions. This architecture is used in sequence transduction models that involve an encoder and a decoder connected through an attention mechanism.\n",
      "\n",
      "================================================================================\n",
      "\n",
      "SOURCE DOCUMENTS USED:\n",
      "================================================================================\n",
      "\n",
      "Document 1:\n",
      "  Source: {'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 12, 'page_label': '13'}\n",
      "  Content: Attention Visualizations\n",
      "Input-Input Layer5\n",
      "It\n",
      "is\n",
      "in\n",
      "this\n",
      "spirit\n",
      "that\n",
      "a\n",
      "majority\n",
      "of\n",
      "American\n",
      "governments\n",
      "have\n",
      "passed\n",
      "new\n",
      "laws\n",
      "since\n",
      "2009\n",
      "making\n",
      "the\n",
      "registration\n",
      "or\n",
      "voting\n",
      "process\n",
      "more\n",
      "difficult\n",
      ".\n",
      "<EOS...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Document 2:\n",
      "  Source: {'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 13, 'page_label': '14'}\n",
      "  Content: Input-Input Layer5\n",
      "The\n",
      "Law\n",
      "will\n",
      "never\n",
      "be\n",
      "perfect\n",
      ",\n",
      "but\n",
      "its\n",
      "application\n",
      "should\n",
      "be\n",
      "just\n",
      "-\n",
      "this\n",
      "is\n",
      "what\n",
      "we\n",
      "are\n",
      "missing\n",
      ",\n",
      "in\n",
      "my\n",
      "opinion\n",
      ".\n",
      "<EOS>\n",
      "<pad>\n",
      "The\n",
      "Law\n",
      "will\n",
      "never\n",
      "be\n",
      "perfect\n",
      ",\n",
      "but\n",
      "its\n",
      "application\n",
      "sh...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Document 3:\n",
      "  Source: {'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 14, 'page_label': '15'}\n",
      "  Content: Input-Input Layer5\n",
      "The\n",
      "Law\n",
      "will\n",
      "never\n",
      "be\n",
      "perfect\n",
      ",\n",
      "but\n",
      "its\n",
      "application\n",
      "should\n",
      "be\n",
      "just\n",
      "-\n",
      "this\n",
      "is\n",
      "what\n",
      "we\n",
      "are\n",
      "missing\n",
      ",\n",
      "in\n",
      "my\n",
      "opinion\n",
      ".\n",
      "<EOS>\n",
      "<pad>\n",
      "The\n",
      "Law\n",
      "will\n",
      "never\n",
      "be\n",
      "perfect\n",
      ",\n",
      "but\n",
      "its\n",
      "application\n",
      "sh...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Document 4:\n",
      "  Source: {'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1'}\n",
      "  Content: Provided proper attribution is provided, Google hereby grants permission to\n",
      "reproduce the tables and figures in this paper solely for use in journalistic or\n",
      "scholarly works.\n",
      "Attention Is All You Need\n",
      "...\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Example Query 1: General question about the document\n",
    "query1 = \"What is the main topic or subject of this document?\"\n",
    "\n",
    "print(f\"Query: {query1}\")\n",
    "print(\"\\nProcessing...\\n\")\n",
    "\n",
    "# With LangChain 1.0+, we invoke the chain with the question directly\n",
    "answer = rag_chain.invoke(query1)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ANSWER:\")\n",
    "print(\"=\" * 80)\n",
    "print(answer)\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "\n",
    "# To see which documents were retrieved, we can call the retriever separately\n",
    "print(\"\\nSOURCE DOCUMENTS USED:\")\n",
    "print(\"=\" * 80)\n",
    "retrieved_docs = retriever.invoke(query1)\n",
    "for i, doc in enumerate(retrieved_docs):\n",
    "    print(f\"\\nDocument {i+1}:\")\n",
    "    print(f\"  Source: {doc.metadata}\")\n",
    "    print(f\"  Content: {doc.page_content[:200]}...\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Can you summarize the key points from this document?\n",
      "\n",
      "Processing...\n",
      "\n",
      "================================================================================\n",
      "ANSWER:\n",
      "================================================================================\n",
      "The document discusses the imperfections of the law and emphasizes that while the law itself may not be perfect, its application should be just. It also mentions that since 2009, a majority of American governments have passed laws that make the registration or voting process more difficult. Additionally, the document includes technical details about attention mechanisms in neural networks, specifically focusing on how attention heads in layer 5 of a 6-layer encoder model are involved in tasks like anaphora resolution and tracking long-distance dependencies in sentences. The document is part of a larger work related to the Transformer model, a network architecture that relies solely on attention mechanisms, eliminating the need for recurrence and convolutions in sequence transduction models.\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Example Query 2: Specific information extraction\n",
    "query2 = \"Can you summarize the key points from this document?\"\n",
    "\n",
    "print(f\"Query: {query2}\")\n",
    "print(\"\\nProcessing...\\n\")\n",
    "\n",
    "answer = rag_chain.invoke(query2)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ANSWER:\")\n",
    "print(\"=\" * 80)\n",
    "print(answer)\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Please you Execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What specific details are mentioned about attention mechanisms?\n",
      "\n",
      "Processing...\n",
      "\n",
      "================================================================================\n",
      "ANSWER:\n",
      "================================================================================\n",
      "The provided context mentions several specific details about attention mechanisms:\n",
      "\n",
      "1. **Self-attention**: This is an attention mechanism that relates different positions of a single sequence to compute a representation of the sequence. It has been used successfully in tasks like reading comprehension, abstractive summarization, textual entailment, and learning task-independent sentence representations.\n",
      "\n",
      "2. **Multi-Head Attention**: This is used to counteract the averaging of attention-weighted positions. It involves multiple attention heads which can attend to different parts of the sequence simultaneously, allowing for a richer representation.\n",
      "\n",
      "3. **End-to-end memory networks**: These use a recurrent attention mechanism instead of sequence-aligned recurrence and have shown good performance in simple-language question answering and language modeling tasks.\n",
      "\n",
      "4. **Transformer model**: This model relies entirely on self-attention to compute representations of its input and output, dispensing with recurrence and convolutions. This is a shift from traditional models that used complex recurrent or convolutional neural networks.\n",
      "\n",
      "These details highlight the versatility and effectiveness of attention mechanisms in various neural network architectures and tasks.\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Example Query 3: Your custom question\n",
    "# Replace this with your own question!\n",
    "custom_query = \"What specific details are mentioned about attention mechanisms?\"\n",
    "\n",
    "print(f\"Query: {custom_query}\")\n",
    "print(\"\\nProcessing...\\n\")\n",
    "\n",
    "answer = rag_chain.invoke(custom_query)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ANSWER:\")\n",
    "print(\"=\" * 80)\n",
    "print(answer)\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What are the applications of Attention Mechanism?\n",
      "\n",
      "Processing...\n",
      "\n",
      "================================================================================\n",
      "ANSWER:\n",
      "================================================================================\n",
      "The applications of the attention mechanism include reading comprehension, abstractive summarization, textual entailment, learning task-independent sentence representations, simple-language question answering, and language modeling tasks.\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Example Query 3: Your custom question\n",
    "# Replace this with your own question!\n",
    "custom_query = \"What are the applications of Attention Mechanism?\"\n",
    "\n",
    "print(f\"Query: {custom_query}\")\n",
    "print(\"\\nProcessing...\\n\")\n",
    "\n",
    "response3 = rag_chain.invoke(custom_query)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ANSWER:\")\n",
    "print(\"=\" * 80)\n",
    "print(response3)\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FAISS Kernel Crash ISSUE\n",
    "\n",
    "Testing retrieval with query: 'What is the main topic of this document?'\n",
    "OMP: Error #15: Initializing libomp.dylib, but found libomp.dylib already initialized.\n",
    "OMP: Hint This means that multiple copies of the OpenMP runtime have been linked into the program. That is dangerous, since it can degrade performance or cause incorrect results. The best thing to do is to ensure that only a single OpenMP runtime is linked into the process, e.g. by avoiding static linking of the OpenMP runtime in any library. As an unsafe, unsupported, undocumented workaround you can set the environment variable KMP_DUPLICATE_LIB_OK=TRUE to allow the program to continue to execute, but that may cause crashes or silently produce incorrect results. For more information, please see http://openmp.llvm.org/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
